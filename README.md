# Back Propogation Network (Gradient Descent Method)

Project Status: Completed

Project Objective:
The main objective of this project was to understand the working of Back Propogation Network (Gradient Descent Method) in Neural network by manually coding each and every step in the process of Back Propogation Network and observing the change in weights as the model train itself to get the desired results

Method Used:
Gradient descent method

Technology:
R 

Project Description:
This is a part of Neural Netork. In this project a Feed Forward network is built and the weights are updated using Back Propogation Network. It is a usual theoretical problem which is taught in neural network books to understand the working of Back Propogation Network, which is automated in this project using R to understand the ways the weights are updated, how do Sigmoidal Activation function is used to calculate the output, the effect of learning rate on the model's ability to achieve desired results. The question which is automated is:
Using back propogation network find the new weights for the following figure. It is representated with the input pattern (0,1) & the target ouput is 1. Use a learning rate alpha =0.25 & apply binary sigmoidal activation function.
